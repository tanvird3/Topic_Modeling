{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'could',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would',\n",
       " 'able',\n",
       " 'abst',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'adj',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'afterwards',\n",
       " 'ah',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'announce',\n",
       " 'another',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apparently',\n",
       " 'approximately',\n",
       " 'arent',\n",
       " 'arise',\n",
       " 'around',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'auth',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'back',\n",
       " 'became',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'beforehand',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'beyond',\n",
       " 'biol',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'came',\n",
       " 'cannot',\n",
       " \"can't\",\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'couldnt',\n",
       " 'date',\n",
       " 'different',\n",
       " 'done',\n",
       " 'downwards',\n",
       " 'due',\n",
       " 'e',\n",
       " 'ed',\n",
       " 'edu',\n",
       " 'effect',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'eighty',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'enough',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'ff',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'found',\n",
       " 'four',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'h',\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'hed',\n",
       " 'hence',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'heres',\n",
       " 'hereupon',\n",
       " 'hes',\n",
       " 'hi',\n",
       " 'hid',\n",
       " 'hither',\n",
       " 'home',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'id',\n",
       " 'ie',\n",
       " 'im',\n",
       " 'immediate',\n",
       " 'immediately',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'index',\n",
       " 'information',\n",
       " 'instead',\n",
       " 'invention',\n",
       " 'inward',\n",
       " 'itd',\n",
       " \"it'll\",\n",
       " 'j',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'kg',\n",
       " 'km',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'little',\n",
       " \"'ll\",\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'mainly',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meantime',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'mg',\n",
       " 'might',\n",
       " 'million',\n",
       " 'miss',\n",
       " 'ml',\n",
       " 'moreover',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'mug',\n",
       " 'must',\n",
       " 'n',\n",
       " 'na',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nay',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessarily',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'ninety',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nonetheless',\n",
       " 'noone',\n",
       " 'normally',\n",
       " 'nos',\n",
       " 'noted',\n",
       " 'nothing',\n",
       " 'nowhere',\n",
       " 'obtain',\n",
       " 'obtained',\n",
       " 'obviously',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omitted',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'onto',\n",
       " 'ord',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'outside',\n",
       " 'overall',\n",
       " 'owing',\n",
       " 'p',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'part',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'past',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'poorly',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'potentially',\n",
       " 'pp',\n",
       " 'predominantly',\n",
       " 'present',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'probably',\n",
       " 'promptly',\n",
       " 'proud',\n",
       " 'provides',\n",
       " 'put',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quickly',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'ran',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 'readily',\n",
       " 'really',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'ref',\n",
       " 'refs',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'related',\n",
       " 'relatively',\n",
       " 'research',\n",
       " 'respectively',\n",
       " 'resulted',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'right',\n",
       " 'run',\n",
       " 'said',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'sec',\n",
       " 'section',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sent',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'shed',\n",
       " 'shes',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'shown',\n",
       " 'showns',\n",
       " 'shows',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'similar',\n",
       " 'similarly',\n",
       " 'since',\n",
       " 'six',\n",
       " 'slightly',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'somethan',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specifically',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'strongly',\n",
       " 'sub',\n",
       " 'substantially',\n",
       " 'successfully',\n",
       " 'sufficiently',\n",
       " 'suggest',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'thats',\n",
       " \"that've\",\n",
       " 'thence',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'thered',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " \"there'll\",\n",
       " 'thereof',\n",
       " 'therere',\n",
       " 'theres',\n",
       " 'thereto',\n",
       " 'thereupon',\n",
       " \"there've\",\n",
       " 'theyd',\n",
       " 'theyre',\n",
       " 'think',\n",
       " 'thou',\n",
       " 'though',\n",
       " 'thoughh',\n",
       " 'thousand',\n",
       " 'throug',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'til',\n",
       " 'tip',\n",
       " 'together',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'ts',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlike',\n",
       " 'unlikely',\n",
       " 'unto',\n",
       " 'upon',\n",
       " 'ups',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usefully',\n",
       " 'usefulness',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " \"'ve\",\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vol',\n",
       " 'vols',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'wasnt',\n",
       " 'way',\n",
       " 'wed',\n",
       " 'welcome',\n",
       " 'went',\n",
       " 'werent',\n",
       " 'whatever',\n",
       " \"what'll\",\n",
       " 'whats',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'wheres',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'whim',\n",
       " 'whither',\n",
       " 'whod',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " \"who'll\",\n",
       " 'whomever',\n",
       " 'whos',\n",
       " 'whose',\n",
       " 'widely',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wont',\n",
       " 'words',\n",
       " 'world',\n",
       " 'wouldnt',\n",
       " 'www',\n",
       " 'x',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'youd',\n",
       " 'youre',\n",
       " 'z',\n",
       " 'zero',\n",
       " \"a's\",\n",
       " \"ain't\",\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'associated',\n",
       " 'best',\n",
       " 'better',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'cant',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'corresponding',\n",
       " 'course',\n",
       " 'currently',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'entirely',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'going',\n",
       " 'greetings',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hopefully',\n",
       " 'ignored',\n",
       " 'inasmuch',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " \"it'd\",\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'novel',\n",
       " 'presumably',\n",
       " 'reasonably',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'sensible',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'sure',\n",
       " \"t's\",\n",
       " 'third',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'three',\n",
       " 'well',\n",
       " 'wonder']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the words those are needed to be skipped\n",
    "stop_words_file = open(\"F:\\GoogleDrive-2\\DS Courses\\Applied Data Science in Python\\Text Mining\\My Codes\\stop_words.txt\", \"r\")\n",
    "stop_words = stop_words_file.read().replace('\\n', '').replace('\"', '').split(',')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of documents\n",
    "with open('newsgroups', 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The best group to keep you informed is the Crohn's and Colitis Foundation\\nof America.  I do not know if the UK has a similar organization.  The\\naddress of\\nthe CCFA is \\n\\nCCFA\\n444 Park Avenue South\\n11th Floor\\nNew York, NY  10016-7374\\nUSA\\n\\nThey have a lot of information available and have a number of newsletters.\\n \\nGood Luck.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizor to find three letter or more tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "#vect = CountVectorizer(min_df=20, max_df=0.2, stop_words=frozenset(stop_words), \n",
    "                      #token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "\n",
    "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words= frozenset(stop_words), \n",
    "                       token_pattern='[A-Za-z]{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['mon'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2000x749 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29022 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform\n",
    "X = vect.fit_transform(newsgroup_data)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
    "# LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"cs\" + 0.016*\"side\" + 0.015*\"post\" + 0.013*\"time\" + 0.013*\"net\" + 0.012*\"doctor\" + 0.011*\"weeks\" + 0.011*\"uk\" + 0.011*\"ac\" + 0.011*\"men\"'),\n",
       " (1,\n",
       "  '0.027*\"people\" + 0.025*\"problems\" + 0.024*\"life\" + 0.018*\"moon\" + 0.018*\"idea\" + 0.017*\"long\" + 0.014*\"good\" + 0.013*\"great\" + 0.013*\"question\" + 0.013*\"break\"'),\n",
       " (2,\n",
       "  '0.043*\"ground\" + 0.036*\"current\" + 0.020*\"good\" + 0.017*\"circuit\" + 0.014*\"hope\" + 0.014*\"year\" + 0.012*\"output\" + 0.012*\"high\" + 0.012*\"noise\" + 0.012*\"turn\"'),\n",
       " (3,\n",
       "  '0.028*\"good\" + 0.026*\"mhz\" + 0.023*\"chip\" + 0.021*\"speed\" + 0.020*\"work\" + 0.017*\"auto\" + 0.017*\"switch\" + 0.016*\"find\" + 0.015*\"manual\" + 0.015*\"motherboard\"'),\n",
       " (4,\n",
       "  '0.025*\"car\" + 0.024*\"system\" + 0.022*\"time\" + 0.020*\"work\" + 0.016*\"oil\" + 0.013*\"problem\" + 0.011*\"dos\" + 0.011*\"port\" + 0.011*\"info\" + 0.010*\"type\"'),\n",
       " (5,\n",
       "  '0.059*\"god\" + 0.039*\"people\" + 0.025*\"argument\" + 0.020*\"evidence\" + 0.019*\"atheism\" + 0.019*\"true\" + 0.018*\"religion\" + 0.014*\"posting\" + 0.012*\"subject\" + 0.012*\"response\"'),\n",
       " (6,\n",
       "  '0.090*\"car\" + 0.045*\"bike\" + 0.036*\"good\" + 0.036*\"cars\" + 0.028*\"engine\" + 0.025*\"miles\" + 0.020*\"driving\" + 0.018*\"lot\" + 0.017*\"mph\" + 0.017*\"bikes\"'),\n",
       " (7,\n",
       "  '0.090*\"space\" + 0.046*\"nasa\" + 0.034*\"data\" + 0.021*\"shuttle\" + 0.019*\"ftp\" + 0.019*\"system\" + 0.017*\"mail\" + 0.016*\"sci\" + 0.016*\"launch\" + 0.016*\"email\"'),\n",
       " (8,\n",
       "  '0.033*\"faq\" + 0.030*\"pain\" + 0.030*\"mark\" + 0.029*\"mike\" + 0.025*\"alt\" + 0.025*\"atheism\" + 0.022*\"send\" + 0.021*\"steve\" + 0.020*\"john\" + 0.019*\"dave\"'),\n",
       " (9,\n",
       "  '0.043*\"science\" + 0.042*\"period\" + 0.032*\"ny\" + 0.027*\"data\" + 0.026*\"air\" + 0.025*\"power\" + 0.024*\"washington\" + 0.023*\"box\" + 0.022*\"thinking\" + 0.018*\"city\"'),\n",
       " (10,\n",
       "  '0.054*\"apple\" + 0.039*\"mb\" + 0.039*\"cd\" + 0.035*\"memory\" + 0.033*\"simms\" + 0.021*\"access\" + 0.020*\"drive\" + 0.020*\"backup\" + 0.016*\"technology\" + 0.015*\"cover\"'),\n",
       " (11,\n",
       "  '0.051*\"pin\" + 0.042*\"perfect\" + 0.033*\"video\" + 0.030*\"sense\" + 0.029*\"connector\" + 0.028*\"pins\" + 0.026*\"correct\" + 0.025*\"thought\" + 0.024*\"apple\" + 0.023*\"monitor\"'),\n",
       " (12,\n",
       "  '0.038*\"time\" + 0.033*\"april\" + 0.031*\"stuff\" + 0.031*\"check\" + 0.022*\"test\" + 0.022*\"years\" + 0.018*\"guess\" + 0.018*\"deleted\" + 0.017*\"ago\" + 0.017*\"friend\"'),\n",
       " (13,\n",
       "  '0.026*\"thing\" + 0.020*\"pitt\" + 0.020*\"time\" + 0.020*\"problem\" + 0.019*\"gordon\" + 0.019*\"banks\" + 0.015*\"good\" + 0.014*\"doctor\" + 0.014*\"geb\" + 0.014*\"surrender\"'),\n",
       " (14,\n",
       "  '0.038*\"scsi\" + 0.032*\"bit\" + 0.031*\"card\" + 0.022*\"monitor\" + 0.016*\"pc\" + 0.016*\"speed\" + 0.015*\"cards\" + 0.013*\"sell\" + 0.012*\"ram\" + 0.012*\"power\"'),\n",
       " (15,\n",
       "  '0.082*\"drive\" + 0.059*\"disk\" + 0.054*\"drives\" + 0.047*\"controller\" + 0.039*\"hard\" + 0.036*\"rom\" + 0.036*\"system\" + 0.034*\"card\" + 0.032*\"floppy\" + 0.027*\"st\"'),\n",
       " (16,\n",
       "  '0.023*\"university\" + 0.023*\"time\" + 0.022*\"medical\" + 0.021*\"center\" + 0.017*\"people\" + 0.013*\"study\" + 0.013*\"high\" + 0.012*\"low\" + 0.012*\"dr\" + 0.012*\"national\"'),\n",
       " (17,\n",
       "  '0.047*\"gm\" + 0.029*\"people\" + 0.026*\"system\" + 0.024*\"front\" + 0.020*\"work\" + 0.019*\"understand\" + 0.018*\"things\" + 0.015*\"food\" + 0.015*\"problem\" + 0.014*\"sort\"'),\n",
       " (18,\n",
       "  '0.040*\"game\" + 0.034*\"year\" + 0.034*\"team\" + 0.023*\"games\" + 0.019*\"play\" + 0.018*\"good\" + 0.017*\"season\" + 0.017*\"players\" + 0.016*\"win\" + 0.015*\"hockey\"'),\n",
       " (19,\n",
       "  '0.045*\"cable\" + 0.041*\"mac\" + 0.041*\"problem\" + 0.035*\"hard\" + 0.031*\"disk\" + 0.028*\"drive\" + 0.025*\"internal\" + 0.024*\"power\" + 0.023*\"advance\" + 0.016*\"external\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using ldamodel, find a list of the 10 topics and the most significant 10 words in each topic. \n",
    "# those numbers are the relative importance of each word in the topic. The reason they don't add upto 1 \n",
    "# is because by default print_topics show 10. \n",
    "# If you show 100 or so the sum will start getting close to 1\n",
    "ldamodel.print_topics(20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because old box use did much damage \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc1=[\"5G is a wireless technology offering an enormous increase in transmission bandwidth, around 10 gigabits per second (GBit/s). Hence, downloading large data in a few seconds is directly pointing to Big Data. This will cause tremendous mobile traffic which may affect the network performance badly. Here comes the need for Cloud Computing technologies to be integrated with mobile systems to help in facilitating computing intensive services.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc2=[\"The government mulls over increasing the cash incentive for exports by one percentage point as it looks to motivate exporters to leverage the sudden opportunities presented by the US-China trade war. Currently, 26 sectors are provided with cash incentives ranging from 2 percent to 20 percent of their export\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc3=[\"The team were accorded a reception by the students as they entered the premises, before making their way down a slope that led to an expansive field with three centre wickets. Mashrafe, who arrived in Cardiff on Thursday night after attending the official captains’ press conference in the afternoon, was the only one of the 15 who did not physically take part in the practice but the inspirational leader was seen discussing strategy with the coach and later cracking jokes with teammates on the sidelines.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc4=[\"China berated the United States for  “bullying” Huawei on Thursday as Panasonic joined a parade of foreign companies reviewing their ties with the telecom giant after a US ban linked to security concerns.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distribution(doc_to_be_tested):\n",
    "    \n",
    "    # Your Code Here\n",
    "    \n",
    "     # Fit and transform\n",
    "    X = vect.transform(doc_to_be_tested)\n",
    "\n",
    "    # Convert sparse matrix to gensim corpus.\n",
    "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "    output = list(ldamodel[corpus])[0]\n",
    "    \n",
    "    return (output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.38621557), (7, 0.5319615)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution(new_doc1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
